//+build !nocuda

package cuda

var kernelPTX = `
//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-21313162
// Cuda compilation tools, release 8.0, V8.0.53
// Based on LLVM 3.4svn
//

.version 5.0
.target sm_20
.address_size 64

	// .globl	divElements
.extern .shared .align 4 .b8 chunk[];
.const .align 4 .b8 __cudart_i2opi_f[24] = {65, 144, 67, 60, 153, 149, 98, 219, 192, 221, 52, 245, 209, 87, 39, 252, 41, 21, 68, 78, 110, 131, 249, 162};

.visible .entry divElements(
	.param .u64 divElements_param_0,
	.param .u64 divElements_param_1,
	.param .u64 divElements_param_2
)
{
	.reg .pred 	%p<2>;
	.reg .f32 	%f<4>;
	.reg .b32 	%r<5>;
	.reg .b64 	%rd<10>;


	ld.param.u64 	%rd2, [divElements_param_0];
	ld.param.u64 	%rd3, [divElements_param_1];
	ld.param.u64 	%rd4, [divElements_param_2];
	mov.u32 	%r1, %ctaid.x;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %tid.x;
	mad.lo.s32 	%r4, %r2, %r1, %r3;
	cvt.u64.u32	%rd1, %r4;
	setp.ge.u64	%p1, %rd1, %rd4;
	@%p1 bra 	BB0_2;

	cvta.to.global.u64 	%rd5, %rd2;
	shl.b64 	%rd6, %rd1, 2;
	add.s64 	%rd7, %rd5, %rd6;
	cvta.to.global.u64 	%rd8, %rd3;
	add.s64 	%rd9, %rd8, %rd6;
	ld.global.f32 	%f1, [%rd9];
	ld.global.f32 	%f2, [%rd7];
	div.rn.f32 	%f3, %f2, %f1;
	st.global.f32 	[%rd7], %f3;

BB0_2:
	ret;
}

	// .globl	expElements
.visible .entry expElements(
	.param .u64 expElements_param_0,
	.param .u64 expElements_param_1
)
{
	.reg .pred 	%p<4>;
	.reg .f32 	%f<15>;
	.reg .b32 	%r<5>;
	.reg .b64 	%rd<7>;


	ld.param.u64 	%rd2, [expElements_param_0];
	ld.param.u64 	%rd3, [expElements_param_1];
	mov.u32 	%r1, %ctaid.x;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %tid.x;
	mad.lo.s32 	%r4, %r2, %r1, %r3;
	cvt.u64.u32	%rd1, %r4;
	setp.ge.u64	%p1, %rd1, %rd3;
	@%p1 bra 	BB1_2;

	cvta.to.global.u64 	%rd4, %rd2;
	shl.b64 	%rd5, %rd1, 2;
	add.s64 	%rd6, %rd4, %rd5;
	ld.global.f32 	%f3, [%rd6];
	mul.f32 	%f4, %f3, 0f3FB8AA3B;
	cvt.rzi.f32.f32	%f5, %f4;
	mov.f32 	%f6, 0fBF317200;
	fma.rn.f32 	%f7, %f5, %f6, %f3;
	mov.f32 	%f8, 0fB5BFBE8E;
	fma.rn.f32 	%f9, %f5, %f8, %f7;
	mul.f32 	%f2, %f9, 0f3FB8AA3B;
	// inline asm
	ex2.approx.ftz.f32 %f1,%f2;
	// inline asm
	add.f32 	%f10, %f5, 0f00000000;
	ex2.approx.f32 	%f11, %f10;
	mul.f32 	%f12, %f1, %f11;
	setp.lt.f32	%p2, %f3, 0fC2D20000;
	selp.f32	%f13, 0f00000000, %f12, %p2;
	setp.gt.f32	%p3, %f3, 0f42D20000;
	selp.f32	%f14, 0f7F800000, %f13, %p3;
	st.global.f32 	[%rd6], %f14;

BB1_2:
	ret;
}

	// .globl	logElements
.visible .entry logElements(
	.param .u64 logElements_param_0,
	.param .u64 logElements_param_1
)
{
	.reg .pred 	%p<5>;
	.reg .f32 	%f<36>;
	.reg .b32 	%r<9>;
	.reg .b64 	%rd<7>;


	ld.param.u64 	%rd3, [logElements_param_0];
	ld.param.u64 	%rd4, [logElements_param_1];
	mov.u32 	%r1, %ctaid.x;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %tid.x;
	mad.lo.s32 	%r4, %r2, %r1, %r3;
	cvt.u64.u32	%rd1, %r4;
	setp.ge.u64	%p1, %rd1, %rd4;
	@%p1 bra 	BB2_4;

	cvta.to.global.u64 	%rd5, %rd3;
	shl.b64 	%rd6, %rd1, 2;
	add.s64 	%rd2, %rd5, %rd6;
	ld.global.f32 	%f5, [%rd2];
	setp.lt.f32	%p2, %f5, 0f00800000;
	mul.f32 	%f6, %f5, 0f4B000000;
	selp.f32	%f1, %f6, %f5, %p2;
	selp.f32	%f7, 0fC1B80000, 0f00000000, %p2;
	mov.b32 	 %r5, %f1;
	add.s32 	%r6, %r5, -1059760811;
	and.b32  	%r7, %r6, -8388608;
	sub.s32 	%r8, %r5, %r7;
	mov.b32 	 %f8, %r8;
	cvt.rn.f32.s32	%f9, %r7;
	mov.f32 	%f10, 0f34000000;
	fma.rn.f32 	%f11, %f9, %f10, %f7;
	add.f32 	%f12, %f8, 0fBF800000;
	mov.f32 	%f13, 0f3E1039F6;
	mov.f32 	%f14, 0fBE055027;
	fma.rn.f32 	%f15, %f14, %f12, %f13;
	mov.f32 	%f16, 0fBDF8CDCC;
	fma.rn.f32 	%f17, %f15, %f12, %f16;
	mov.f32 	%f18, 0f3E0F2955;
	fma.rn.f32 	%f19, %f17, %f12, %f18;
	mov.f32 	%f20, 0fBE2AD8B9;
	fma.rn.f32 	%f21, %f19, %f12, %f20;
	mov.f32 	%f22, 0f3E4CED0B;
	fma.rn.f32 	%f23, %f21, %f12, %f22;
	mov.f32 	%f24, 0fBE7FFF22;
	fma.rn.f32 	%f25, %f23, %f12, %f24;
	mov.f32 	%f26, 0f3EAAAA78;
	fma.rn.f32 	%f27, %f25, %f12, %f26;
	mov.f32 	%f28, 0fBF000000;
	fma.rn.f32 	%f29, %f27, %f12, %f28;
	mul.f32 	%f30, %f12, %f29;
	fma.rn.f32 	%f31, %f30, %f12, %f12;
	mov.f32 	%f32, 0f3F317218;
	fma.rn.f32 	%f35, %f11, %f32, %f31;
	setp.lt.u32	%p3, %r5, 2139095040;
	@%p3 bra 	BB2_3;

	mov.f32 	%f33, 0f7F800000;
	fma.rn.f32 	%f35, %f1, %f33, %f33;

BB2_3:
	setp.eq.f32	%p4, %f1, 0f00000000;
	selp.f32	%f34, 0fFF800000, %f35, %p4;
	st.global.f32 	[%rd2], %f34;

BB2_4:
	ret;
}

	// .globl	tanhElements
.visible .entry tanhElements(
	.param .u64 tanhElements_param_0,
	.param .u64 tanhElements_param_1
)
{
	.reg .pred 	%p<5>;
	.reg .f32 	%f<33>;
	.reg .b32 	%r<10>;
	.reg .b64 	%rd<7>;


	ld.param.u64 	%rd3, [tanhElements_param_0];
	ld.param.u64 	%rd4, [tanhElements_param_1];
	mov.u32 	%r1, %ctaid.x;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %tid.x;
	mad.lo.s32 	%r4, %r2, %r1, %r3;
	cvt.u64.u32	%rd1, %r4;
	setp.ge.u64	%p1, %rd1, %rd4;
	@%p1 bra 	BB3_5;

	cvta.to.global.u64 	%rd5, %rd3;
	shl.b64 	%rd6, %rd1, 2;
	add.s64 	%rd2, %rd5, %rd6;
	ld.global.f32 	%f1, [%rd2];
	abs.f32 	%f2, %f1;
	setp.ltu.f32	%p2, %f2, 0f3F0CCCCD;
	@%p2 bra 	BB3_3;
	bra.uni 	BB3_2;

BB3_3:
	mul.f32 	%f21, %f1, %f1;
	mov.f32 	%f22, 0fBD57BE66;
	mov.f32 	%f23, 0f3C86A81B;
	fma.rn.f32 	%f24, %f23, %f21, %f22;
	mov.f32 	%f25, 0f3E08677B;
	fma.rn.f32 	%f26, %f24, %f21, %f25;
	mov.f32 	%f27, 0fBEAAAA29;
	fma.rn.f32 	%f28, %f26, %f21, %f27;
	mul.f32 	%f29, %f21, %f28;
	fma.rn.f32 	%f30, %f29, %f1, %f1;
	add.f32 	%f31, %f1, %f1;
	setp.eq.f32	%p4, %f1, 0f00000000;
	selp.f32	%f32, %f31, %f30, %p4;
	bra.uni 	BB3_4;

BB3_2:
	add.f32 	%f10, %f2, %f2;
	mul.f32 	%f11, %f10, 0f3FB8AA3B;
	cvt.rzi.f32.f32	%f12, %f11;
	mov.f32 	%f13, 0fBF317200;
	fma.rn.f32 	%f14, %f12, %f13, %f10;
	mov.f32 	%f15, 0fB5BFBE8E;
	fma.rn.f32 	%f16, %f12, %f15, %f14;
	mul.f32 	%f7, %f16, 0f3FB8AA3B;
	// inline asm
	ex2.approx.ftz.f32 %f6,%f7;
	// inline asm
	ex2.approx.f32 	%f17, %f12;
	mov.f32 	%f18, 0f3F800000;
	fma.rn.f32 	%f9, %f6, %f17, %f18;
	// inline asm
	rcp.approx.ftz.f32 %f8,%f9;
	// inline asm
	mov.f32 	%f19, 0fC0000000;
	fma.rn.f32 	%f20, %f8, %f19, %f18;
	mov.b32 	 %r5, %f20;
	setp.ltu.f32	%p3, %f2, 0f42B00000;
	selp.b32	%r6, %r5, 1065353216, %p3;
	mov.b32 	 %r7, %f1;
	and.b32  	%r8, %r7, -2147483648;
	or.b32  	%r9, %r6, %r8;
	mov.b32 	 %f32, %r9;

BB3_4:
	st.global.f32 	[%rd2], %f32;

BB3_5:
	ret;
}

	// .globl	sinElements
.visible .entry sinElements(
	.param .u64 sinElements_param_0,
	.param .u64 sinElements_param_1
)
{
	.local .align 4 .b8 	__local_depot4[28];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<15>;
	.reg .f32 	%f<48>;
	.reg .b32 	%r<96>;
	.reg .b64 	%rd<19>;


	mov.u64 	%rd18, __local_depot4;
	cvta.local.u64 	%SP, %rd18;
	ld.param.u64 	%rd9, [sinElements_param_0];
	ld.param.u64 	%rd10, [sinElements_param_1];
	mov.u32 	%r36, %ntid.x;
	mov.u32 	%r37, %ctaid.x;
	mov.u32 	%r38, %tid.x;
	mad.lo.s32 	%r39, %r36, %r37, %r38;
	cvt.u64.u32	%rd1, %r39;
	setp.ge.u64	%p1, %rd1, %rd10;
	@%p1 bra 	BB4_24;

	cvta.to.global.u64 	%rd11, %rd9;
	shl.b64 	%rd12, %rd1, 2;
	add.s64 	%rd2, %rd11, %rd12;
	add.u64 	%rd13, %SP, 0;
	cvta.to.local.u64 	%rd3, %rd13;
	ld.global.f32 	%f43, [%rd2];
	abs.f32 	%f19, %f43;
	setp.neu.f32	%p2, %f19, 0f7F800000;
	@%p2 bra 	BB4_3;

	mov.f32 	%f20, 0f00000000;
	mul.rn.f32 	%f43, %f43, %f20;

BB4_3:
	mul.f32 	%f21, %f43, 0f3F22F983;
	cvt.rni.s32.f32	%r95, %f21;
	cvt.rn.f32.s32	%f22, %r95;
	neg.f32 	%f23, %f22;
	mov.f32 	%f24, 0f3FC90FDA;
	fma.rn.f32 	%f25, %f23, %f24, %f43;
	mov.f32 	%f26, 0f33A22168;
	fma.rn.f32 	%f27, %f23, %f26, %f25;
	mov.f32 	%f28, 0f27C234C5;
	fma.rn.f32 	%f44, %f23, %f28, %f27;
	abs.f32 	%f29, %f43;
	setp.leu.f32	%p3, %f29, 0f47CE4780;
	@%p3 bra 	BB4_13;

	mov.b32 	 %r2, %f43;
	shr.u32 	%r3, %r2, 23;
	shl.b32 	%r42, %r2, 8;
	or.b32  	%r4, %r42, -2147483648;
	mov.u32 	%r87, 0;
	mov.u64 	%rd16, __cudart_i2opi_f;
	mov.u32 	%r86, -6;
	mov.u64 	%rd17, %rd3;

BB4_5:
	.pragma "nounroll";
	mov.u64 	%rd5, %rd17;
	ld.const.u32 	%r45, [%rd16];
	// inline asm
	{
	mad.lo.cc.u32   %r43, %r45, %r4, %r87;
	madc.hi.u32     %r87, %r45, %r4,  0;
	}
	// inline asm
	st.local.u32 	[%rd5], %r43;
	add.s64 	%rd6, %rd5, 4;
	add.s64 	%rd16, %rd16, 4;
	add.s32 	%r86, %r86, 1;
	setp.ne.s32	%p4, %r86, 0;
	mov.u64 	%rd17, %rd6;
	@%p4 bra 	BB4_5;

	and.b32  	%r48, %r3, 255;
	add.s32 	%r49, %r48, -128;
	shr.u32 	%r50, %r49, 5;
	and.b32  	%r9, %r2, -2147483648;
	st.local.u32 	[%rd3+24], %r87;
	mov.u32 	%r51, 6;
	sub.s32 	%r52, %r51, %r50;
	mul.wide.s32 	%rd15, %r52, 4;
	add.s64 	%rd8, %rd3, %rd15;
	ld.local.u32 	%r88, [%rd8];
	ld.local.u32 	%r89, [%rd8+-4];
	and.b32  	%r12, %r3, 31;
	setp.eq.s32	%p5, %r12, 0;
	@%p5 bra 	BB4_8;

	mov.u32 	%r53, 32;
	sub.s32 	%r54, %r53, %r12;
	shr.u32 	%r55, %r89, %r54;
	shl.b32 	%r56, %r88, %r12;
	add.s32 	%r88, %r55, %r56;
	ld.local.u32 	%r57, [%rd8+-8];
	shr.u32 	%r58, %r57, %r54;
	shl.b32 	%r59, %r89, %r12;
	add.s32 	%r89, %r58, %r59;

BB4_8:
	shr.u32 	%r60, %r89, 30;
	shl.b32 	%r61, %r88, 2;
	add.s32 	%r90, %r60, %r61;
	shl.b32 	%r18, %r89, 2;
	shr.u32 	%r62, %r90, 31;
	shr.u32 	%r63, %r88, 30;
	add.s32 	%r19, %r62, %r63;
	setp.eq.s32	%p6, %r62, 0;
	mov.u32 	%r91, %r9;
	mov.u32 	%r92, %r18;
	@%p6 bra 	BB4_10;

	not.b32 	%r64, %r90;
	neg.s32 	%r20, %r18;
	setp.eq.s32	%p7, %r18, 0;
	selp.u32	%r65, 1, 0, %p7;
	add.s32 	%r90, %r65, %r64;
	xor.b32  	%r22, %r9, -2147483648;
	mov.u32 	%r91, %r22;
	mov.u32 	%r92, %r20;

BB4_10:
	mov.u32 	%r24, %r91;
	neg.s32 	%r66, %r19;
	setp.eq.s32	%p8, %r9, 0;
	selp.b32	%r95, %r19, %r66, %p8;
	clz.b32 	%r94, %r90;
	setp.eq.s32	%p9, %r94, 0;
	shl.b32 	%r67, %r90, %r94;
	mov.u32 	%r68, 32;
	sub.s32 	%r69, %r68, %r94;
	shr.u32 	%r70, %r92, %r69;
	add.s32 	%r71, %r70, %r67;
	selp.b32	%r28, %r90, %r71, %p9;
	mov.u32 	%r72, -921707870;
	mul.hi.u32 	%r93, %r28, %r72;
	setp.lt.s32	%p10, %r93, 1;
	@%p10 bra 	BB4_12;

	mul.lo.s32 	%r73, %r28, -921707870;
	shr.u32 	%r74, %r73, 31;
	shl.b32 	%r75, %r93, 1;
	add.s32 	%r93, %r74, %r75;
	add.s32 	%r94, %r94, 1;

BB4_12:
	mov.u32 	%r76, 126;
	sub.s32 	%r77, %r76, %r94;
	shl.b32 	%r78, %r77, 23;
	add.s32 	%r79, %r93, 1;
	shr.u32 	%r80, %r79, 7;
	add.s32 	%r81, %r80, 1;
	shr.u32 	%r82, %r81, 1;
	add.s32 	%r83, %r82, %r78;
	or.b32  	%r84, %r83, %r24;
	mov.b32 	 %f44, %r84;

BB4_13:
	mul.rn.f32 	%f7, %f44, %f44;
	and.b32  	%r35, %r95, 1;
	setp.eq.s32	%p11, %r35, 0;
	@%p11 bra 	BB4_15;

	mov.f32 	%f30, 0fBAB6061A;
	mov.f32 	%f31, 0f37CCF5CE;
	fma.rn.f32 	%f45, %f31, %f7, %f30;
	bra.uni 	BB4_16;

BB4_15:
	mov.f32 	%f32, 0f3C08839E;
	mov.f32 	%f33, 0fB94CA1F9;
	fma.rn.f32 	%f45, %f33, %f7, %f32;

BB4_16:
	@%p11 bra 	BB4_18;

	mov.f32 	%f34, 0f3D2AAAA5;
	fma.rn.f32 	%f35, %f45, %f7, %f34;
	mov.f32 	%f36, 0fBF000000;
	fma.rn.f32 	%f46, %f35, %f7, %f36;
	bra.uni 	BB4_19;

BB4_18:
	mov.f32 	%f37, 0fBE2AAAA3;
	fma.rn.f32 	%f38, %f45, %f7, %f37;
	mov.f32 	%f39, 0f00000000;
	fma.rn.f32 	%f46, %f38, %f7, %f39;

BB4_19:
	fma.rn.f32 	%f47, %f46, %f44, %f44;
	@%p11 bra 	BB4_21;

	mov.f32 	%f40, 0f3F800000;
	fma.rn.f32 	%f47, %f46, %f7, %f40;

BB4_21:
	and.b32  	%r85, %r95, 2;
	setp.eq.s32	%p14, %r85, 0;
	@%p14 bra 	BB4_23;

	mov.f32 	%f41, 0f00000000;
	mov.f32 	%f42, 0fBF800000;
	fma.rn.f32 	%f47, %f47, %f42, %f41;

BB4_23:
	st.global.f32 	[%rd2], %f47;

BB4_24:
	ret;
}

	// .globl	clipPositive
.visible .entry clipPositive(
	.param .u64 clipPositive_param_0,
	.param .u64 clipPositive_param_1
)
{
	.reg .pred 	%p<2>;
	.reg .f32 	%f<4>;
	.reg .b32 	%r<5>;
	.reg .b64 	%rd<7>;


	ld.param.u64 	%rd2, [clipPositive_param_0];
	ld.param.u64 	%rd3, [clipPositive_param_1];
	mov.u32 	%r1, %ctaid.x;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %tid.x;
	mad.lo.s32 	%r4, %r2, %r1, %r3;
	cvt.u64.u32	%rd1, %r4;
	setp.ge.u64	%p1, %rd1, %rd3;
	@%p1 bra 	BB5_2;

	cvta.to.global.u64 	%rd4, %rd2;
	shl.b64 	%rd5, %rd1, 2;
	add.s64 	%rd6, %rd4, %rd5;
	ld.global.f32 	%f1, [%rd6];
	mov.f32 	%f2, 0f00000000;
	max.f32 	%f3, %f2, %f1;
	st.global.f32 	[%rd6], %f3;

BB5_2:
	ret;
}

	// .globl	shiftRandUniform
.visible .entry shiftRandUniform(
	.param .u64 shiftRandUniform_param_0,
	.param .u64 shiftRandUniform_param_1
)
{
	.reg .pred 	%p<3>;
	.reg .f32 	%f<2>;
	.reg .b32 	%r<6>;
	.reg .b64 	%rd<7>;


	ld.param.u64 	%rd3, [shiftRandUniform_param_0];
	ld.param.u64 	%rd4, [shiftRandUniform_param_1];
	mov.u32 	%r1, %ctaid.x;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %tid.x;
	mad.lo.s32 	%r4, %r2, %r1, %r3;
	cvt.u64.u32	%rd1, %r4;
	setp.ge.u64	%p1, %rd1, %rd4;
	@%p1 bra 	BB6_3;

	cvta.to.global.u64 	%rd5, %rd3;
	shl.b64 	%rd6, %rd1, 2;
	add.s64 	%rd2, %rd5, %rd6;
	ld.global.f32 	%f1, [%rd2];
	setp.neu.f32	%p2, %f1, 0f3F800000;
	@%p2 bra 	BB6_3;

	mov.u32 	%r5, 0;
	st.global.u32 	[%rd2], %r5;

BB6_3:
	ret;
}

	// .globl	uniformToBernoulli
.visible .entry uniformToBernoulli(
	.param .u64 uniformToBernoulli_param_0,
	.param .u64 uniformToBernoulli_param_1
)
{
	.reg .pred 	%p<3>;
	.reg .f32 	%f<2>;
	.reg .b32 	%r<7>;
	.reg .b64 	%rd<7>;


	ld.param.u64 	%rd3, [uniformToBernoulli_param_0];
	ld.param.u64 	%rd4, [uniformToBernoulli_param_1];
	mov.u32 	%r1, %ctaid.x;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %tid.x;
	mad.lo.s32 	%r4, %r2, %r1, %r3;
	cvt.u64.u32	%rd1, %r4;
	setp.ge.u64	%p1, %rd1, %rd4;
	@%p1 bra 	BB7_4;

	cvta.to.global.u64 	%rd5, %rd3;
	shl.b64 	%rd6, %rd1, 2;
	add.s64 	%rd2, %rd5, %rd6;
	ld.global.f32 	%f1, [%rd2];
	setp.gt.f32	%p2, %f1, 0f3F000000;
	@%p2 bra 	BB7_3;
	bra.uni 	BB7_2;

BB7_3:
	mov.u32 	%r6, 1065353216;
	st.global.u32 	[%rd2], %r6;
	bra.uni 	BB7_4;

BB7_2:
	mov.u32 	%r5, 0;
	st.global.u32 	[%rd2], %r5;

BB7_4:
	ret;
}

	// .globl	addRepeated
.visible .entry addRepeated(
	.param .u64 addRepeated_param_0,
	.param .u64 addRepeated_param_1,
	.param .u64 addRepeated_param_2,
	.param .u64 addRepeated_param_3
)
{
	.reg .pred 	%p<3>;
	.reg .f32 	%f<4>;
	.reg .b32 	%r<8>;
	.reg .b64 	%rd<17>;


	ld.param.u64 	%rd5, [addRepeated_param_0];
	ld.param.u64 	%rd6, [addRepeated_param_1];
	ld.param.u64 	%rd8, [addRepeated_param_2];
	ld.param.u64 	%rd7, [addRepeated_param_3];
	mov.u32 	%r1, %ctaid.x;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %tid.x;
	mad.lo.s32 	%r4, %r2, %r1, %r3;
	cvt.u64.u32	%rd1, %r4;
	setp.ge.u64	%p1, %rd1, %rd8;
	@%p1 bra 	BB8_5;

	and.b64  	%rd9, %rd7, -4294967296;
	setp.eq.s64	%p2, %rd9, 0;
	@%p2 bra 	BB8_3;

	rem.u64 	%rd16, %rd1, %rd7;
	bra.uni 	BB8_4;

BB8_3:
	cvt.u32.u64	%r5, %rd7;
	cvt.u32.u64	%r6, %rd1;
	rem.u32 	%r7, %r6, %r5;
	cvt.u64.u32	%rd16, %r7;

BB8_4:
	cvta.to.global.u64 	%rd10, %rd6;
	cvta.to.global.u64 	%rd11, %rd5;
	shl.b64 	%rd12, %rd1, 2;
	add.s64 	%rd13, %rd11, %rd12;
	ld.global.f32 	%f1, [%rd13];
	shl.b64 	%rd14, %rd16, 2;
	add.s64 	%rd15, %rd10, %rd14;
	ld.global.f32 	%f2, [%rd15];
	add.f32 	%f3, %f2, %f1;
	st.global.f32 	[%rd13], %f3;

BB8_5:
	ret;
}

	// .globl	addRepeatedPow2
.visible .entry addRepeatedPow2(
	.param .u64 addRepeatedPow2_param_0,
	.param .u64 addRepeatedPow2_param_1,
	.param .u64 addRepeatedPow2_param_2,
	.param .u64 addRepeatedPow2_param_3
)
{
	.reg .pred 	%p<2>;
	.reg .f32 	%f<4>;
	.reg .b32 	%r<5>;
	.reg .b64 	%rd<13>;


	ld.param.u64 	%rd2, [addRepeatedPow2_param_0];
	ld.param.u64 	%rd3, [addRepeatedPow2_param_1];
	ld.param.u64 	%rd5, [addRepeatedPow2_param_2];
	ld.param.u64 	%rd4, [addRepeatedPow2_param_3];
	mov.u32 	%r1, %ctaid.x;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %tid.x;
	mad.lo.s32 	%r4, %r2, %r1, %r3;
	cvt.u64.u32	%rd1, %r4;
	setp.ge.u64	%p1, %rd1, %rd5;
	@%p1 bra 	BB9_2;

	cvta.to.global.u64 	%rd6, %rd3;
	and.b64  	%rd7, %rd1, %rd4;
	shl.b64 	%rd8, %rd7, 2;
	add.s64 	%rd9, %rd6, %rd8;
	cvta.to.global.u64 	%rd10, %rd2;
	shl.b64 	%rd11, %rd1, 2;
	add.s64 	%rd12, %rd10, %rd11;
	ld.global.f32 	%f1, [%rd12];
	ld.global.f32 	%f2, [%rd9];
	add.f32 	%f3, %f2, %f1;
	st.global.f32 	[%rd12], %f3;

BB9_2:
	ret;
}

	// .globl	scaleRepeated
.visible .entry scaleRepeated(
	.param .u64 scaleRepeated_param_0,
	.param .u64 scaleRepeated_param_1,
	.param .u64 scaleRepeated_param_2,
	.param .u64 scaleRepeated_param_3
)
{
	.reg .pred 	%p<3>;
	.reg .f32 	%f<4>;
	.reg .b32 	%r<8>;
	.reg .b64 	%rd<17>;


	ld.param.u64 	%rd5, [scaleRepeated_param_0];
	ld.param.u64 	%rd6, [scaleRepeated_param_1];
	ld.param.u64 	%rd8, [scaleRepeated_param_2];
	ld.param.u64 	%rd7, [scaleRepeated_param_3];
	mov.u32 	%r1, %ctaid.x;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %tid.x;
	mad.lo.s32 	%r4, %r2, %r1, %r3;
	cvt.u64.u32	%rd1, %r4;
	setp.ge.u64	%p1, %rd1, %rd8;
	@%p1 bra 	BB10_5;

	and.b64  	%rd9, %rd7, -4294967296;
	setp.eq.s64	%p2, %rd9, 0;
	@%p2 bra 	BB10_3;

	rem.u64 	%rd16, %rd1, %rd7;
	bra.uni 	BB10_4;

BB10_3:
	cvt.u32.u64	%r5, %rd7;
	cvt.u32.u64	%r6, %rd1;
	rem.u32 	%r7, %r6, %r5;
	cvt.u64.u32	%rd16, %r7;

BB10_4:
	cvta.to.global.u64 	%rd10, %rd6;
	cvta.to.global.u64 	%rd11, %rd5;
	shl.b64 	%rd12, %rd1, 2;
	add.s64 	%rd13, %rd11, %rd12;
	ld.global.f32 	%f1, [%rd13];
	shl.b64 	%rd14, %rd16, 2;
	add.s64 	%rd15, %rd10, %rd14;
	ld.global.f32 	%f2, [%rd15];
	mul.f32 	%f3, %f2, %f1;
	st.global.f32 	[%rd13], %f3;

BB10_5:
	ret;
}

	// .globl	scaleRepeatedPow2
.visible .entry scaleRepeatedPow2(
	.param .u64 scaleRepeatedPow2_param_0,
	.param .u64 scaleRepeatedPow2_param_1,
	.param .u64 scaleRepeatedPow2_param_2,
	.param .u64 scaleRepeatedPow2_param_3
)
{
	.reg .pred 	%p<2>;
	.reg .f32 	%f<4>;
	.reg .b32 	%r<5>;
	.reg .b64 	%rd<13>;


	ld.param.u64 	%rd2, [scaleRepeatedPow2_param_0];
	ld.param.u64 	%rd3, [scaleRepeatedPow2_param_1];
	ld.param.u64 	%rd5, [scaleRepeatedPow2_param_2];
	ld.param.u64 	%rd4, [scaleRepeatedPow2_param_3];
	mov.u32 	%r1, %ctaid.x;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %tid.x;
	mad.lo.s32 	%r4, %r2, %r1, %r3;
	cvt.u64.u32	%rd1, %r4;
	setp.ge.u64	%p1, %rd1, %rd5;
	@%p1 bra 	BB11_2;

	cvta.to.global.u64 	%rd6, %rd3;
	and.b64  	%rd7, %rd1, %rd4;
	shl.b64 	%rd8, %rd7, 2;
	add.s64 	%rd9, %rd6, %rd8;
	cvta.to.global.u64 	%rd10, %rd2;
	shl.b64 	%rd11, %rd1, 2;
	add.s64 	%rd12, %rd10, %rd11;
	ld.global.f32 	%f1, [%rd12];
	ld.global.f32 	%f2, [%rd9];
	mul.f32 	%f3, %f2, %f1;
	st.global.f32 	[%rd12], %f3;

BB11_2:
	ret;
}

	// .globl	addScaler
.visible .entry addScaler(
	.param .f32 addScaler_param_0,
	.param .u64 addScaler_param_1,
	.param .u64 addScaler_param_2
)
{
	.reg .pred 	%p<2>;
	.reg .f32 	%f<4>;
	.reg .b32 	%r<5>;
	.reg .b64 	%rd<7>;


	ld.param.f32 	%f1, [addScaler_param_0];
	ld.param.u64 	%rd2, [addScaler_param_1];
	ld.param.u64 	%rd3, [addScaler_param_2];
	mov.u32 	%r1, %ctaid.x;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %tid.x;
	mad.lo.s32 	%r4, %r2, %r1, %r3;
	cvt.u64.u32	%rd1, %r4;
	setp.ge.u64	%p1, %rd1, %rd3;
	@%p1 bra 	BB12_2;

	cvta.to.global.u64 	%rd4, %rd2;
	shl.b64 	%rd5, %rd1, 2;
	add.s64 	%rd6, %rd4, %rd5;
	ld.global.f32 	%f2, [%rd6];
	add.f32 	%f3, %f2, %f1;
	st.global.f32 	[%rd6], %f3;

BB12_2:
	ret;
}

	// .globl	addChunks
.visible .entry addChunks(
	.param .u64 addChunks_param_0,
	.param .u64 addChunks_param_1,
	.param .u64 addChunks_param_2,
	.param .u64 addChunks_param_3
)
{
	.reg .pred 	%p<3>;
	.reg .f32 	%f<4>;
	.reg .b32 	%r<8>;
	.reg .b64 	%rd<17>;


	ld.param.u64 	%rd5, [addChunks_param_0];
	ld.param.u64 	%rd6, [addChunks_param_1];
	ld.param.u64 	%rd8, [addChunks_param_2];
	ld.param.u64 	%rd7, [addChunks_param_3];
	mov.u32 	%r1, %ctaid.x;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %tid.x;
	mad.lo.s32 	%r4, %r2, %r1, %r3;
	cvt.u64.u32	%rd1, %r4;
	setp.ge.u64	%p1, %rd1, %rd8;
	@%p1 bra 	BB13_5;

	and.b64  	%rd9, %rd7, -4294967296;
	setp.eq.s64	%p2, %rd9, 0;
	@%p2 bra 	BB13_3;

	div.u64 	%rd16, %rd1, %rd7;
	bra.uni 	BB13_4;

BB13_3:
	cvt.u32.u64	%r5, %rd7;
	cvt.u32.u64	%r6, %rd1;
	div.u32 	%r7, %r6, %r5;
	cvt.u64.u32	%rd16, %r7;

BB13_4:
	cvta.to.global.u64 	%rd10, %rd6;
	cvta.to.global.u64 	%rd11, %rd5;
	shl.b64 	%rd12, %rd1, 2;
	add.s64 	%rd13, %rd11, %rd12;
	ld.global.f32 	%f1, [%rd13];
	shl.b64 	%rd14, %rd16, 2;
	add.s64 	%rd15, %rd10, %rd14;
	ld.global.f32 	%f2, [%rd15];
	add.f32 	%f3, %f2, %f1;
	st.global.f32 	[%rd13], %f3;

BB13_5:
	ret;
}

	// .globl	subChunks
.visible .entry subChunks(
	.param .u64 subChunks_param_0,
	.param .u64 subChunks_param_1,
	.param .u64 subChunks_param_2,
	.param .u64 subChunks_param_3
)
{
	.reg .pred 	%p<3>;
	.reg .f32 	%f<4>;
	.reg .b32 	%r<8>;
	.reg .b64 	%rd<17>;


	ld.param.u64 	%rd5, [subChunks_param_0];
	ld.param.u64 	%rd6, [subChunks_param_1];
	ld.param.u64 	%rd8, [subChunks_param_2];
	ld.param.u64 	%rd7, [subChunks_param_3];
	mov.u32 	%r1, %ctaid.x;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %tid.x;
	mad.lo.s32 	%r4, %r2, %r1, %r3;
	cvt.u64.u32	%rd1, %r4;
	setp.ge.u64	%p1, %rd1, %rd8;
	@%p1 bra 	BB14_5;

	and.b64  	%rd9, %rd7, -4294967296;
	setp.eq.s64	%p2, %rd9, 0;
	@%p2 bra 	BB14_3;

	div.u64 	%rd16, %rd1, %rd7;
	bra.uni 	BB14_4;

BB14_3:
	cvt.u32.u64	%r5, %rd7;
	cvt.u32.u64	%r6, %rd1;
	div.u32 	%r7, %r6, %r5;
	cvt.u64.u32	%rd16, %r7;

BB14_4:
	cvta.to.global.u64 	%rd10, %rd6;
	cvta.to.global.u64 	%rd11, %rd5;
	shl.b64 	%rd12, %rd1, 2;
	add.s64 	%rd13, %rd11, %rd12;
	ld.global.f32 	%f1, [%rd13];
	shl.b64 	%rd14, %rd16, 2;
	add.s64 	%rd15, %rd10, %rd14;
	ld.global.f32 	%f2, [%rd15];
	sub.f32 	%f3, %f1, %f2;
	st.global.f32 	[%rd13], %f3;

BB14_5:
	ret;
}

	// .globl	lessThan
.visible .entry lessThan(
	.param .f32 lessThan_param_0,
	.param .u64 lessThan_param_1,
	.param .u64 lessThan_param_2
)
{
	.reg .pred 	%p<3>;
	.reg .f32 	%f<3>;
	.reg .b32 	%r<7>;
	.reg .b64 	%rd<7>;


	ld.param.f32 	%f1, [lessThan_param_0];
	ld.param.u64 	%rd3, [lessThan_param_1];
	ld.param.u64 	%rd4, [lessThan_param_2];
	mov.u32 	%r1, %ctaid.x;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %tid.x;
	mad.lo.s32 	%r4, %r2, %r1, %r3;
	cvt.u64.u32	%rd1, %r4;
	setp.ge.u64	%p1, %rd1, %rd4;
	@%p1 bra 	BB15_4;

	cvta.to.global.u64 	%rd5, %rd3;
	shl.b64 	%rd6, %rd1, 2;
	add.s64 	%rd2, %rd5, %rd6;
	ld.global.f32 	%f2, [%rd2];
	setp.lt.f32	%p2, %f2, %f1;
	@%p2 bra 	BB15_3;
	bra.uni 	BB15_2;

BB15_3:
	mov.u32 	%r6, 1065353216;
	st.global.u32 	[%rd2], %r6;
	bra.uni 	BB15_4;

BB15_2:
	mov.u32 	%r5, 0;
	st.global.u32 	[%rd2], %r5;

BB15_4:
	ret;
}

	// .globl	greaterThan
.visible .entry greaterThan(
	.param .f32 greaterThan_param_0,
	.param .u64 greaterThan_param_1,
	.param .u64 greaterThan_param_2
)
{
	.reg .pred 	%p<3>;
	.reg .f32 	%f<3>;
	.reg .b32 	%r<7>;
	.reg .b64 	%rd<7>;


	ld.param.f32 	%f1, [greaterThan_param_0];
	ld.param.u64 	%rd3, [greaterThan_param_1];
	ld.param.u64 	%rd4, [greaterThan_param_2];
	mov.u32 	%r1, %ctaid.x;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %tid.x;
	mad.lo.s32 	%r4, %r2, %r1, %r3;
	cvt.u64.u32	%rd1, %r4;
	setp.ge.u64	%p1, %rd1, %rd4;
	@%p1 bra 	BB16_4;

	cvta.to.global.u64 	%rd5, %rd3;
	shl.b64 	%rd6, %rd1, 2;
	add.s64 	%rd2, %rd5, %rd6;
	ld.global.f32 	%f2, [%rd2];
	setp.gt.f32	%p2, %f2, %f1;
	@%p2 bra 	BB16_3;
	bra.uni 	BB16_2;

BB16_3:
	mov.u32 	%r6, 1065353216;
	st.global.u32 	[%rd2], %r6;
	bra.uni 	BB16_4;

BB16_2:
	mov.u32 	%r5, 0;
	st.global.u32 	[%rd2], %r5;

BB16_4:
	ret;
}

	// .globl	equalTo
.visible .entry equalTo(
	.param .f32 equalTo_param_0,
	.param .u64 equalTo_param_1,
	.param .u64 equalTo_param_2
)
{
	.reg .pred 	%p<3>;
	.reg .f32 	%f<3>;
	.reg .b32 	%r<7>;
	.reg .b64 	%rd<7>;


	ld.param.f32 	%f1, [equalTo_param_0];
	ld.param.u64 	%rd3, [equalTo_param_1];
	ld.param.u64 	%rd4, [equalTo_param_2];
	mov.u32 	%r1, %ctaid.x;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %tid.x;
	mad.lo.s32 	%r4, %r2, %r1, %r3;
	cvt.u64.u32	%rd1, %r4;
	setp.ge.u64	%p1, %rd1, %rd4;
	@%p1 bra 	BB17_4;

	cvta.to.global.u64 	%rd5, %rd3;
	shl.b64 	%rd6, %rd1, 2;
	add.s64 	%rd2, %rd5, %rd6;
	ld.global.f32 	%f2, [%rd2];
	setp.eq.f32	%p2, %f2, %f1;
	@%p2 bra 	BB17_3;
	bra.uni 	BB17_2;

BB17_3:
	mov.u32 	%r6, 1065353216;
	st.global.u32 	[%rd2], %r6;
	bra.uni 	BB17_4;

BB17_2:
	mov.u32 	%r5, 0;
	st.global.u32 	[%rd2], %r5;

BB17_4:
	ret;
}

	// .globl	addLogs
.visible .entry addLogs(
	.param .u64 addLogs_param_0,
	.param .u64 addLogs_param_1,
	.param .u64 addLogs_param_2
)
{
	.reg .pred 	%p<15>;
	.reg .f32 	%f<68>;
	.reg .b32 	%r<14>;
	.reg .b64 	%rd<27>;


	ld.param.u64 	%rd7, [addLogs_param_0];
	ld.param.u64 	%rd8, [addLogs_param_1];
	ld.param.u64 	%rd9, [addLogs_param_2];
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r2, %ctaid.x;
	mov.u32 	%r3, %tid.x;
	mad.lo.s32 	%r5, %r1, %r2, %r3;
	cvt.u64.u32	%rd1, %r5;
	cvt.u64.u32	%rd2, %r3;
	mul.wide.u32 	%rd10, %r3, 4;
	mov.u64 	%rd11, chunk;
	add.s64 	%rd3, %rd11, %rd10;
	setp.ge.u64	%p1, %rd1, %rd9;
	@%p1 bra 	BB18_2;

	cvta.to.global.u64 	%rd12, %rd8;
	mov.u32 	%r6, %ctaid.y;
	cvt.u64.u32	%rd13, %r6;
	mul.lo.s64 	%rd14, %rd13, %rd9;
	add.s64 	%rd15, %rd14, %rd1;
	shl.b64 	%rd16, %rd15, 2;
	add.s64 	%rd17, %rd12, %rd16;
	ld.global.f32 	%f6, [%rd17];
	st.shared.f32 	[%rd3], %f6;

BB18_2:
	bar.sync 	0;
	shr.u32 	%r4, %r1, 1;
	setp.eq.s32	%p2, %r4, 0;
	@%p2 bra 	BB18_9;

	cvt.u64.u32	%rd26, %r4;

BB18_4:
	add.s64 	%rd18, %rd26, %rd1;
	setp.lt.u64	%p3, %rd18, %rd9;
	setp.lt.u64	%p4, %rd2, %rd26;
	and.pred  	%p5, %p4, %p3;
	@!%p5 bra 	BB18_8;
	bra.uni 	BB18_5;

BB18_5:
	ld.shared.f32 	%f11, [%rd3];
	add.s64 	%rd19, %rd26, %rd2;
	shl.b64 	%rd20, %rd19, 2;
	add.s64 	%rd22, %rd11, %rd20;
	ld.shared.f32 	%f12, [%rd22];
	max.f32 	%f1, %f11, %f12;
	sub.f32 	%f13, %f11, %f1;
	mul.f32 	%f14, %f13, 0f3FB8AA3B;
	cvt.rzi.f32.f32	%f15, %f14;
	mov.f32 	%f16, 0fBF317200;
	fma.rn.f32 	%f17, %f15, %f16, %f13;
	mov.f32 	%f18, 0fB5BFBE8E;
	fma.rn.f32 	%f19, %f15, %f18, %f17;
	mul.f32 	%f8, %f19, 0f3FB8AA3B;
	// inline asm
	ex2.approx.ftz.f32 %f7,%f8;
	// inline asm
	add.f32 	%f20, %f15, 0f00000000;
	ex2.approx.f32 	%f21, %f20;
	mul.f32 	%f22, %f7, %f21;
	setp.lt.f32	%p6, %f13, 0fC2D20000;
	selp.f32	%f23, 0f00000000, %f22, %p6;
	setp.gt.f32	%p7, %f13, 0f42D20000;
	selp.f32	%f24, 0f7F800000, %f23, %p7;
	sub.f32 	%f25, %f12, %f1;
	mul.f32 	%f26, %f25, 0f3FB8AA3B;
	cvt.rzi.f32.f32	%f27, %f26;
	fma.rn.f32 	%f28, %f27, %f16, %f25;
	fma.rn.f32 	%f29, %f27, %f18, %f28;
	mul.f32 	%f10, %f29, 0f3FB8AA3B;
	// inline asm
	ex2.approx.ftz.f32 %f9,%f10;
	// inline asm
	add.f32 	%f30, %f27, 0f00000000;
	ex2.approx.f32 	%f31, %f30;
	mul.f32 	%f32, %f9, %f31;
	setp.lt.f32	%p8, %f25, 0fC2D20000;
	selp.f32	%f33, 0f00000000, %f32, %p8;
	setp.gt.f32	%p9, %f25, 0f42D20000;
	selp.f32	%f34, 0f7F800000, %f33, %p9;
	add.f32 	%f35, %f24, %f34;
	setp.lt.f32	%p10, %f35, 0f00800000;
	mul.f32 	%f36, %f35, 0f4B000000;
	selp.f32	%f2, %f36, %f35, %p10;
	selp.f32	%f37, 0fC1B80000, 0f00000000, %p10;
	mov.b32 	 %r7, %f2;
	add.s32 	%r8, %r7, -1059760811;
	and.b32  	%r9, %r8, -8388608;
	sub.s32 	%r10, %r7, %r9;
	mov.b32 	 %f38, %r10;
	cvt.rn.f32.s32	%f39, %r9;
	mov.f32 	%f40, 0f34000000;
	fma.rn.f32 	%f41, %f39, %f40, %f37;
	add.f32 	%f42, %f38, 0fBF800000;
	mov.f32 	%f43, 0f3E1039F6;
	mov.f32 	%f44, 0fBE055027;
	fma.rn.f32 	%f45, %f44, %f42, %f43;
	mov.f32 	%f46, 0fBDF8CDCC;
	fma.rn.f32 	%f47, %f45, %f42, %f46;
	mov.f32 	%f48, 0f3E0F2955;
	fma.rn.f32 	%f49, %f47, %f42, %f48;
	mov.f32 	%f50, 0fBE2AD8B9;
	fma.rn.f32 	%f51, %f49, %f42, %f50;
	mov.f32 	%f52, 0f3E4CED0B;
	fma.rn.f32 	%f53, %f51, %f42, %f52;
	mov.f32 	%f54, 0fBE7FFF22;
	fma.rn.f32 	%f55, %f53, %f42, %f54;
	mov.f32 	%f56, 0f3EAAAA78;
	fma.rn.f32 	%f57, %f55, %f42, %f56;
	mov.f32 	%f58, 0fBF000000;
	fma.rn.f32 	%f59, %f57, %f42, %f58;
	mul.f32 	%f60, %f59, %f42;
	fma.rn.f32 	%f61, %f60, %f42, %f42;
	mov.f32 	%f62, 0f3F317218;
	fma.rn.f32 	%f67, %f41, %f62, %f61;
	setp.lt.u32	%p11, %r7, 2139095040;
	@%p11 bra 	BB18_7;

	mov.f32 	%f63, 0f7F800000;
	fma.rn.f32 	%f67, %f2, %f63, %f63;

BB18_7:
	setp.eq.f32	%p12, %f2, 0f00000000;
	selp.f32	%f64, 0fFF800000, %f67, %p12;
	add.f32 	%f65, %f1, %f64;
	st.shared.f32 	[%rd3], %f65;

BB18_8:
	bar.sync 	0;
	shr.u64 	%rd26, %rd26, 1;
	setp.ne.s64	%p13, %rd26, 0;
	@%p13 bra 	BB18_4;

BB18_9:
	setp.ne.s32	%p14, %r3, 0;
	@%p14 bra 	BB18_11;

	cvta.to.global.u64 	%rd23, %rd7;
	ld.shared.f32 	%f66, [chunk];
	mov.u32 	%r11, %nctaid.x;
	mov.u32 	%r12, %ctaid.y;
	mad.lo.s32 	%r13, %r11, %r12, %r2;
	mul.wide.u32 	%rd24, %r13, 4;
	add.s64 	%rd25, %rd23, %rd24;
	st.global.f32 	[%rd25], %f66;

BB18_11:
	ret;
}

	// .globl	powScaler
.visible .entry powScaler(
	.param .f32 powScaler_param_0,
	.param .u64 powScaler_param_1,
	.param .u64 powScaler_param_2
)
{
	.reg .pred 	%p<30>;
	.reg .f32 	%f<103>;
	.reg .b32 	%r<24>;
	.reg .b64 	%rd<7>;


	ld.param.f32 	%f17, [powScaler_param_0];
	ld.param.u64 	%rd3, [powScaler_param_1];
	ld.param.u64 	%rd4, [powScaler_param_2];
	mov.u32 	%r1, %ctaid.x;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %tid.x;
	mad.lo.s32 	%r4, %r2, %r1, %r3;
	cvt.u64.u32	%rd1, %r4;
	setp.ge.u64	%p2, %rd1, %rd4;
	@%p2 bra 	BB19_15;

	cvta.to.global.u64 	%rd5, %rd3;
	shl.b64 	%rd6, %rd1, 2;
	add.s64 	%rd2, %rd5, %rd6;
	mul.f32 	%f22, %f17, 0f3F000000;
	cvt.rzi.f32.f32	%f23, %f22;
	fma.rn.f32 	%f24, %f23, 0fC0000000, %f17;
	abs.f32 	%f1, %f24;
	ld.global.f32 	%f2, [%rd2];
	abs.f32 	%f3, %f2;
	setp.lt.f32	%p3, %f3, 0f00800000;
	mul.f32 	%f25, %f3, 0f4B800000;
	selp.f32	%f26, 0fC3170000, 0fC2FE0000, %p3;
	selp.f32	%f27, %f25, %f3, %p3;
	mov.b32 	 %r5, %f27;
	and.b32  	%r6, %r5, 8388607;
	or.b32  	%r7, %r6, 1065353216;
	mov.b32 	 %f28, %r7;
	shr.u32 	%r8, %r5, 23;
	cvt.rn.f32.u32	%f29, %r8;
	add.f32 	%f30, %f26, %f29;
	setp.gt.f32	%p4, %f28, 0f3FB504F3;
	mul.f32 	%f31, %f28, 0f3F000000;
	add.f32 	%f32, %f30, 0f3F800000;
	selp.f32	%f33, %f31, %f28, %p4;
	selp.f32	%f34, %f32, %f30, %p4;
	add.f32 	%f35, %f33, 0fBF800000;
	add.f32 	%f19, %f33, 0f3F800000;
	// inline asm
	rcp.approx.ftz.f32 %f18,%f19;
	// inline asm
	add.f32 	%f36, %f35, %f35;
	mul.f32 	%f37, %f18, %f36;
	mul.f32 	%f38, %f37, %f37;
	mov.f32 	%f39, 0f3C4CAF63;
	mov.f32 	%f40, 0f3B18F0FE;
	fma.rn.f32 	%f41, %f40, %f38, %f39;
	mov.f32 	%f42, 0f3DAAAABD;
	fma.rn.f32 	%f43, %f41, %f38, %f42;
	mul.rn.f32 	%f44, %f43, %f38;
	mul.rn.f32 	%f45, %f44, %f37;
	sub.f32 	%f46, %f35, %f37;
	neg.f32 	%f47, %f37;
	add.f32 	%f48, %f46, %f46;
	fma.rn.f32 	%f49, %f47, %f35, %f48;
	mul.rn.f32 	%f50, %f18, %f49;
	add.f32 	%f51, %f45, %f37;
	sub.f32 	%f52, %f37, %f51;
	add.f32 	%f53, %f45, %f52;
	add.f32 	%f54, %f50, %f53;
	add.f32 	%f55, %f51, %f54;
	sub.f32 	%f56, %f51, %f55;
	add.f32 	%f57, %f54, %f56;
	mov.f32 	%f58, 0f3F317200;
	mul.rn.f32 	%f59, %f34, %f58;
	mov.f32 	%f60, 0f35BFBE8E;
	mul.rn.f32 	%f61, %f34, %f60;
	add.f32 	%f62, %f59, %f55;
	sub.f32 	%f63, %f59, %f62;
	add.f32 	%f64, %f55, %f63;
	add.f32 	%f65, %f57, %f64;
	add.f32 	%f66, %f61, %f65;
	add.f32 	%f67, %f62, %f66;
	sub.f32 	%f68, %f62, %f67;
	add.f32 	%f69, %f66, %f68;
	abs.f32 	%f4, %f17;
	setp.gt.f32	%p5, %f4, 0f77F684DF;
	mul.f32 	%f70, %f17, 0f39000000;
	selp.f32	%f71, %f70, %f17, %p5;
	mul.rn.f32 	%f72, %f71, %f67;
	neg.f32 	%f73, %f72;
	fma.rn.f32 	%f74, %f71, %f67, %f73;
	fma.rn.f32 	%f75, %f71, %f69, %f74;
	mov.f32 	%f76, 0f00000000;
	fma.rn.f32 	%f77, %f76, %f67, %f75;
	add.rn.f32 	%f78, %f72, %f77;
	neg.f32 	%f79, %f78;
	add.rn.f32 	%f80, %f72, %f79;
	add.rn.f32 	%f81, %f80, %f77;
	mov.b32 	 %r9, %f78;
	setp.eq.s32	%p6, %r9, 1118925336;
	add.s32 	%r10, %r9, -1;
	mov.b32 	 %f82, %r10;
	add.f32 	%f83, %f81, 0f37000000;
	selp.f32	%f84, %f82, %f78, %p6;
	selp.f32	%f5, %f83, %f81, %p6;
	mul.f32 	%f85, %f84, 0f3FB8AA3B;
	cvt.rzi.f32.f32	%f86, %f85;
	mov.f32 	%f87, 0fBF317200;
	fma.rn.f32 	%f88, %f86, %f87, %f84;
	mov.f32 	%f89, 0fB5BFBE8E;
	fma.rn.f32 	%f90, %f86, %f89, %f88;
	mul.f32 	%f21, %f90, 0f3FB8AA3B;
	// inline asm
	ex2.approx.ftz.f32 %f20,%f21;
	// inline asm
	add.f32 	%f91, %f86, 0f00000000;
	ex2.approx.f32 	%f92, %f91;
	mul.f32 	%f93, %f20, %f92;
	setp.lt.f32	%p7, %f84, 0fC2D20000;
	selp.f32	%f94, 0f00000000, %f93, %p7;
	setp.gt.f32	%p8, %f84, 0f42D20000;
	selp.f32	%f101, 0f7F800000, %f94, %p8;
	setp.eq.f32	%p9, %f101, 0f7F800000;
	@%p9 bra 	BB19_3;

	fma.rn.f32 	%f101, %f101, %f5, %f101;

BB19_3:
	setp.lt.f32	%p10, %f2, 0f00000000;
	setp.eq.f32	%p11, %f1, 0f3F800000;
	and.pred  	%p1, %p10, %p11;
	mov.b32 	 %r11, %f101;
	xor.b32  	%r12, %r11, -2147483648;
	mov.b32 	 %f95, %r12;
	selp.f32	%f102, %f95, %f101, %p1;
	setp.eq.f32	%p12, %f2, 0f00000000;
	@%p12 bra 	BB19_6;
	bra.uni 	BB19_4;

BB19_6:
	add.f32 	%f97, %f2, %f2;
	mov.b32 	 %r13, %f97;
	selp.b32	%r14, %r13, 0, %p11;
	or.b32  	%r15, %r14, 2139095040;
	setp.lt.f32	%p16, %f17, 0f00000000;
	selp.b32	%r16, %r15, %r14, %p16;
	mov.b32 	 %f102, %r16;
	bra.uni 	BB19_7;

BB19_4:
	setp.geu.f32	%p13, %f2, 0f00000000;
	@%p13 bra 	BB19_7;

	cvt.rzi.f32.f32	%f96, %f17;
	setp.neu.f32	%p14, %f96, %f17;
	selp.f32	%f102, 0f7FFFFFFF, %f102, %p14;

BB19_7:
	add.f32 	%f98, %f3, %f4;
	mov.b32 	 %r17, %f98;
	setp.lt.s32	%p17, %r17, 2139095040;
	@%p17 bra 	BB19_14;

	setp.gtu.f32	%p18, %f3, 0f7F800000;
	setp.gtu.f32	%p19, %f4, 0f7F800000;
	or.pred  	%p20, %p18, %p19;
	@%p20 bra 	BB19_13;
	bra.uni 	BB19_9;

BB19_13:
	add.f32 	%f102, %f2, %f17;
	bra.uni 	BB19_14;

BB19_9:
	setp.eq.f32	%p21, %f4, 0f7F800000;
	@%p21 bra 	BB19_12;
	bra.uni 	BB19_10;

BB19_12:
	setp.gt.f32	%p24, %f3, 0f3F800000;
	selp.b32	%r21, 2139095040, 0, %p24;
	xor.b32  	%r22, %r21, 2139095040;
	setp.lt.f32	%p25, %f17, 0f00000000;
	selp.b32	%r23, %r22, %r21, %p25;
	mov.b32 	 %f99, %r23;
	setp.eq.f32	%p26, %f2, 0fBF800000;
	selp.f32	%f102, 0f3F800000, %f99, %p26;
	bra.uni 	BB19_14;

BB19_10:
	setp.neu.f32	%p22, %f3, 0f7F800000;
	@%p22 bra 	BB19_14;

	setp.ltu.f32	%p23, %f17, 0f00000000;
	selp.b32	%r18, 0, 2139095040, %p23;
	or.b32  	%r19, %r18, -2147483648;
	selp.b32	%r20, %r19, %r18, %p1;
	mov.b32 	 %f102, %r20;

BB19_14:
	setp.eq.f32	%p27, %f17, 0f00000000;
	setp.eq.f32	%p28, %f2, 0f3F800000;
	or.pred  	%p29, %p28, %p27;
	selp.f32	%f100, 0f3F800000, %f102, %p29;
	st.global.f32 	[%rd2], %f100;

BB19_15:
	ret;
}


`
